---
layout: page
title: Projects
permalink: /projects/
---

*Disclaimer:* I will try to keep this list updated, but please check my github for my latest projects.

## General:

* <a href="https://github.com/jacobaustin123/Coral">**Coral Programming Language**</a> The Coral programming language is a blazingly-fast, gradually typed Python-like language with powerful optional typing for improved safety and performance. Coral performs type inference on optionally typed Python code and seamlessly optimizes type-inferred objects to be nearly as efficient as equivalent C-code, orders of magnitude faster than Python. Coral also enforces types at compile and runtime, catching errors where possible before code is run, and otherwise throwing errors at runtime.

* <a href="https://github.com/jacobaustin123/Titan">**Titan Simulation Library**</a> The Titan simulation library is a versitile CUDA-based physics simulation library that provides a GPU-accelerated environment for physics primatives like springs and masses. Library users can create masses, springs, and more complicated objects, apply constraints, and modify simulation parameters in real time, while the simulation runs asynchronously on the GPU.

## Networks:

* <a href="https://github.com/jacobaustin123/char-rnn-tensorflow">**Char-RNN:**</a> My latest project is a general-purpose character-level Long Short-Term Memory (LSTM) RNN for learning from and interpreting text, which can be trained on any large text dataset with good performance and excellent results.

* <a href="https://github.com/jacobaustin123/tic-tac-toe-minimax">**Tic Tac Toe:**</a> In this repository, I implement a number of reinforcement learning and minimax algorithms for playing tic-tac-toe, including Q-learning, alpha-beta pruning, heuristic evaluation functions, and the classical minimax algorithm.

* <a href="https://github.com/jacobaustin123/reinforcement-learning">**Reinforcement Learning:**</a> These scripts demonstrate solutions to some classic reinforcement learning problems, and solve optimal control problems for OpenAI challenges. The k-armed-bandit script compares and visualizes algorithms for solving the classic problem in reinforcement learning.

* <a href="">**Generative Adversarial Networks (GANs):**</a> These networks, trained on MNIST, CIFAR-10, and several face datasets, are capable of learning to produce unique image data from existing datasets. They work by pitting two neural networks against each other: one, a classifier, tries to tell which image is real and which is fake, while the other tries to generate more realistic images.

* <a href="">**Deep Convolution Classifiers:**</a> These are simple, classical convolution networks trained on MNIST, CIFAR-10, and the ILSVRC datasets, which can classify even complex images with high accuracy into anywhere between 2 and 10000 classes.

* <a href="https://github.com/jacobaustin123/gradient-descent-examples">**Logistic/Linear Regression in Numpy:**</a> These scripts use backpropogation to perform linear and logistic regression, with an algorithm implemented from scratch in Numpy/Python.

Other side-projects include <a href="https://github.com/jacobaustin123/demos/blob/master/python-notes.ipynb">general python notes</a> and <a href="https://github.com/jacobaustin123/demos/blob/master/python-notes-modules.ipynb">module tutorials</a>, and various scientific visualization tools. More small scripts and utilities will be added gradually.

## Tensorflow/Keras Tutorials

* <a href="https://github.com/jacobaustin123/tensorflow-keras-tutorials/blob/master/CNN%20Tutorial%20MNIST.ipynb">**MNIST CNN Classifier Tutorial:**</a> This notebook is a simple tutorial introduction to image classification in Keras, using the MNIST dataset. We cover fully connected networks, various CNNs, and hidden layer visualization to help understand how the model learns.

* <a href="https://github.com/jacobaustin123/tensorflow-keras-tutorials/blob/master/SGD%20Linear%20Regression%20in%20Numpy%20Tutorial.ipynb">**Introduction to SGD, Vectorization, and Linear Regression in Numpy:**</a> In this tutorial, we implement the "backpropogation" algorithm for a simple linear regression model on MNIST, first using an iterative approach (looping over the weights), and then trying a vectorized implementation in Numpy. We talk about the calculus of Stochastic Gradient Descent, and prepare for a logistic regression implementation in the next tutorial.

## Other Work:

* <a href="https://github.com/jacobaustin123/examples/blob/master/preprocessing.py">**Image Preprocessing Script:**</a> A script which preprocesses image data and saves it in convenient formats, with a large number of options including resizing, greyscale conversion, and channel normalization.

* <a href="https://github.com/jacobaustin123/demos/blob/master/mandelbrot.py">**Mandelbrot Generator:**</a> A small Mandelbrot generator, with arbitrary resolution and size.

* <a href="https://github.com/jacobaustin123/demos/blob/master/central_limit.py">**Central Limit Theorem Demonstration:**</a> A small script which demonstrates the converge of the mean of samples from arbitrary distributions to a normal distribution.
