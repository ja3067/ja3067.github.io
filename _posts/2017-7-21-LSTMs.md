---
layout: post
title: Exploring LSTMs
excerpt: <img src="/images/RNN.png">
---

The past week has seen me diving headfirst into the world of Recurrent Neural Networks (or RNNs), networks capable of sequential learning. Humans are amazing at learning because we learn in context; we can remember and contextualize a word in a sentence that last appeared pages ago. Our neural networks should be able to do the same thing. 

RNN inputs are sequences - like sentences, which are sequences of characters or words, or even images, which are sequences of pixels. Despite this apparent complication, they tend to be structurally simpler than the convolution networks used in image classification. 
