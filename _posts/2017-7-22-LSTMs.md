---
layout: post
title: Exploring LSTMs
---

The past week has seen me diving headfirst into the world of Recurrent Neural Networks (or RNNs), networks capable of sequential learning. Humans are amazing at learning because we learn in context; we can remember and contextualize a word in a sentence that last appeared pages ago. Our neural networks should be able to do the same thing. 

RNN inputs are sequences - like sentences, which are sequences of characters or words, or even images, which are sequences of pixels. Despite this apparent complication, they tend to be structurally simpler than the convolution networks used in image classification. The simplest RNN model is shown below: the first element in the sequence is fed into the network, which produces an output *h* and a state vector, represented by the arrow exiting and reentering the body, which is fed back into the same network in time for the next input to be processed. Since the weights inside the neuron "body" control both how it interprets and generates the state vector, it can learn via backpropogation how to summarize and decode important information about previous elements in the sequence.

<img src="/images/RNN-unrolled.png>

Like in convolution networks, the ability of such neural networks to summarize and perform "unsupervised learning" is quite extraordinary, and visualizing the activations of a fully trained LSTM can be linguistically fascinating. For this model, the mathematics is quite simple. The input, say, a character (a-z), is one-hot encoded, i.e. represented as a vector as long as the number of unique inputs (26, in this case), with all zeros except at the position corresponding to the character in question. This vector is concatenated with the state vector, and the combined vector is put through a linear layer (multiplied by a matrix w and shifted by a bias b.

$$
\begin{align*}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
$$
