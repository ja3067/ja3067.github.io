---
layout: post
title: "Effective RNNs: A Guide to Recurrent Neural Networks in Tensorflow"
use_math: true
excerpt: Humans are great at contextualizing information. Our neural networks should be too.<br><br><img src="/images/RNN-unrolled.png"><br><br>
---

<img src="/images/RNN-unrolled.png"><br><br>

# Why RNNs?

I woke up the other day, rather bleary-eyed before my first cup of coffee, and sat down at my computer to check out the RNN I had thrown together the previous evening and left running overnight on the CPU of my Macbook. What I found was, to put it mildly, surprising.

```
CAPULET:
Come, come in, we will tell thee, as reason
To the stately royaltys but little looks,
Where did he stoop and notice thee.

LUSLIS:
To my way, in the thing I did make the contents
That thyself that dissolvath you.

ROMEO:
Undowses, but he shall resolve to be so.

BUCKINGHAM:
That we will find me Henry, his men.

First Musician:
Sir, here’s a pretty thing, our soldiers’ wounds
are from the infants murderd weeping for
thy carelessness.
```

Okay, sure, it's not really Shakespeare, but I'd forgive someone for mistaking it for an obscure quote from one of his later plays. And it wasn't just that quote that struck me, but page after page of faux-Shakespeare from my hastily written network. Andrej Karpathy's famous essay on the subject, [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), is aptly named: *RNNs are uncanny, strange, unpredictable, and sometimes beautiful*. In this tutorial, I'd like to go through the basics of RNNs and LSTMs in particular, provide some tips for implementing and training them, and talk about some of the recent literature and theory. You can find the char-rnn responsible for that passage, and many more pages from this and other datasets in my [Github LSTM repository](https://github.com/ja3067/char-rnn-tensorflow).

# What is an RNN?

RNNs, or Recurrent Neural Networks, are networks capable of sequential learning. Humans are amazing at learning because we learn in context; we can remember and contextualize a word in a sentence that last appeared pages ago. *Our neural networks should be able to do the same thing.*

RNN inputs are sequences, and one thing you soon discover is that **just about anything can be a sequence**. It's easy to see that words and sentences are sequential, but RNNs can also be used to generate images, write music, and improve classification algorithms. [Recent computer vision networks](https://arxiv.org/abs/1412.7755) have even used RNNs to help their networks make smarter choices about what to look at in an image. So let's get started. Here's the basic RNN architecture:

<br><img src="/images/RNN-unrolled.png" style="width: 90%; display: block; margin: 0 auto;"><br>

This may look complicated, but it's actually pretty simple. The RNN is just a single cell with an input, an output, and a state vector, which is updated every time a new input is processed. During training, a sequence of elements is fed one by one into the RNN, and the state vector is continually updated, acting as a sort of record of the past elements in the sequence. Since the weights inside the neuron "body" control both how it interprets and generates the state vector, it can learn via backpropogation how to produce more accurate outputs by better summarizing and decoding important information about previous inputs.

The mathematics of such a model is quite simple. The input, say, a character (a-z), is one-hot encoded, i.e. represented as a vector as long as the number of unique inputs (26, in this case), with all zeros except at the position corresponding to the character in question. The state and input vectors are put through linear layers (multiplied by a matrix w and shifted by a bias b), and then are added to each other and normalized with a tanh activation. This is the state vector, but we need to add one more linear layer to decode the output, since generally the vectors will be longer than the input length while inside the RNN cell.

$$
\begin{align*}
  &s_t = \tanh(w_i\cdot i_t + w_s\cdot s_{t-1} + b_i + b_s)\\
  &h_t = w_o\cdot s_t + b_o
\end{align*}
$$

Our loss function is just an L2 or cross-entropy loss function measuring the distance between the output sequence and the desired results. Usually the labels will actually just be the input sequence shift right by one time-step, so the network learns to produce subsequent sequential elements. Backpropogation is not difficult, although the gradient associated with outputs later in the sequence will be quite long, since they depend on every input layer before them.

## Implementation in Tensorflow

The LSTM cell is just a set of three matrices `wi`, `ws`, and `wo` with biases, applied to the inputs, state vector, and outputs, respectively. A Tensorflow pseudo-code implementation looks like this:

```python
def RNN_cell(input, state):

  wi = tf.get_variable("wi", [input_size, rnn_size], initializer=...) # weight and bias for input
  bi = tf.get_variable("bi", [rnn_size], initializer=...)

  ws = tf.get_variable("ws", [rnn_size, rnn_size], initializer=...) # weight and bias for state vector
  bs = tf.get_variable("bs", [rnn_size], initializer=...)

  wo = tf.get_variable("wo", [rnn_size, vocab_size], initializer=...) # weight and bias for decoding RNN output
  bo = tf.get_variable("bo", [vocab_size], initializer=...)

  state = tf.nn.tanh(tf.nn.xw_plus_b(input, wi, bi) + tf.nn.xw_plus_b(state, ws, bs))
  output = tf.nn.xw_plus_b(state, wo, bo)

  return state, output
```
Easy-peasy. RNNs can also be stacked on top of each other, so the output sequence is used as the input sequence for another RNN. Either way, our labels are just going to be the inputs shifted by one time stamp, since we want our RNN to predict the next element in a sequence. That elegance is what makes RNNs so powerful and easy to use

```python
train_data = list() # here we take a sequence of characters from the trianing data and store them in a list
for _ in range(sequence_length + 1):
  train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))
inputs = train_data[:sequence_length]
train_labels = train_data[1:]
```

Then we create an initial state variable, and repeatedly apply the RNN_cell to our inputs, modifying the state variable each time. At the end, we convert our output list into a tensor, and apply a standard loss function.

```python
state = tf.constant(shape=[rnn_size]) # initial state variable

outputs = [] # list for storing outputs

for element in inputs:
  with tf.variable_scope("RNN") as scope:
    state, output = RNN_cell(element, state)
    scope.reuse_variables()
    outputs.append(output)

logits = tf.pack(outputs) # convert list into tensor

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
                            labels=tf.concat(train_labels, 0), logits=logits)

predictions = tf.nn.softmax(logits)

```

The model is then trained on a standard SGD optimizer (classical SGD, or Adam/RMSProp/others).

## LSTMs: What's the Big Deal?

We have a working model right now, but it has some major drawbacks. For one thing, it's very shallow, and these linear layers aren't capable of capturing much complexity in the state space. For another things, the state vector is very crude, and information about older elements can be quickly lost as it optimizes for shorter-term memory. These problems are rectified by the LSTM (Long Short-Term Memory) model.

An LSTM is an RNN unit with a clever mechanism for learning what to remember and what to forget. To do this, we add aptly-named forget and memory gates, which filter the state vector at every step.

<br><img src="/images/LSTM3-chain.png" style="width: 90%; display: block; margin: 0 auto;"><br>

The yellow squares, in order from left to right, are the input, forget, memory, and output gates, and they each have a specific function.

Like in convolution networks, the ability of such neural networks to summarize and perform "unsupervised learning" is quite extraordinary, and visualizing the activations of a fully trained LSTM can be linguistically fascinating.
