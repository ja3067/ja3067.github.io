---
layout: post
title: Exploring RNNs (Recurrent Neural Networks)
use_math: true
excerpt: <img src="/images/RNN-unrolled.png"><br>
---

This week has seen me diving headfirst into the world of Recurrent Neural Networks (or RNNs), networks capable of sequential learning. Humans are amazing at learning because we learn in context; we can remember and contextualize a word in a sentence that last appeared pages ago. *Our neural networks should be able to do the same thing.* 

Before I begin, I'd like to point everyone towards Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), which has inspired many budding researchers to explore recurrent networks. Even simple RNNs can be uncannily effective at generating quite extraordinary results, and his essay highlights some of the beauty of their use. His Char-RNN model, written in Lua, is also a good resource for implementation advice.

>RNN inputs are sequences, and one thing you soon discover is that *just about anything* can be a sequence.

It's easy to see that words and sentences are sequential, but RNNs can also be used to generate images, write music, and improve classification algorithms. Many recent computer vision networks have used RNNs to help their networks make smarter choices about where to look and what to care about. [Here's](https://arxiv.org/abs/1412.7755) a fascinating example.

Despite the apparent complexity of a sequence based model, *they tend to be structurally simpler* than the large convolution networks used in image classification. The simplest RNN model is shown below: the first element in the sequence is fed into the network, which produces an output *h* and a state vector, represented by the arrow exiting and reentering the body, which is fed back into the same network in time for the next input to be processed. Since the weights inside the neuron "body" control both how it interprets and generates the state vector, it can learn via backpropogation how to summarize and decode important information about previous elements in the sequence.

![Unrolled RNN](/images/RNN-unrolled.png)

The mathematics of such a model is quite simple. The input, say, a character (a-z), is one-hot encoded, i.e. represented as a vector as long as the number of unique inputs (26, in this case), with all zeros except at the position corresponding to the character in question. The state and input vectors are put through linear layers (multiplied by a matrix w and shifted by a bias b), and then are summed and normalized.

$$
\begin{align*}
  s_t = \tanh(w_i\cdot i_t + w_s\cdot s_{t-1} + b_i + b_s)
  h_t = w_o\cdot s_t + b_o
\end{align*}
$$

**The implementation in Tensorflow pseudo-code** is something like this:

First, we define the RNN_cell, which takes our inputs and the previous state, and returns an output and state. We concatenatee the input and state, apply the linear layer, and return a new state variable. The output has to be resized back to the size of the vocabulary, so we push it through yet another linear layer. 


```python
def RNN_cell(input, state):

  wi = tf.get_variable("wi", [input_size, rnn_size], initializer=...) # weight and bias for input
  bi = tf.get_variable("bi", [rnn_size], initializer=...)
  
  ws = tf.get_variable("ws", [rnn_size, rnn_size], initializer=...) # weight and bias for state vector
  bs = tf.get_variable("bs", [rnn_size], initializer=...)
  
  wo = tf.get_variable("wo", [rnn_size, vocab_size], initializer=...) # weight and bias for decoding RNN output
  bo = tf.get_variable("bo", [vocab_size], initializer=...)

  state = tf.nn.tanh(tf.nn.xw_plus_b(input, wi, bi) + tf.nn.xw_plus_b(state, ws, bs))
  output = tf.nn.xw_plus_b(state, wo, bo)
  
  return state, output
```
 
Then we unpack the the training data and create separate data and label tensors. The labels are just going to be the inputs shifted by one time stamp, since we want our RNN to predict the next element in a sequence. That elegance is what makes RNNs so powerful and easy to use
  
```python
train_data = list() # here we take a sequence of characters from the trianing data and store them in a list
for _ in range(sequence_length + 1):
  train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))
inputs = train_data[:sequence_length]
train_labels = train_data[1:]
```

Then we create an initial state variable, and repeatedly apply the RNN_cell to our inputs, modifying the state variable each time. At the end, we convert our output list into a tensor, and apply a standard loss function.

```python
state = tf.constant(shape=[rnn_size]) # initial state variable

outputs = [] # list for storing outputs
 
for element in inputs:
  with tf.variable_scope("RNN") as scope:
    state, output = RNN_cell(element, state)
    scope.reuse_variables()
    outputs.append(output)

logits = tf.pack(outputs) # convert list into tensor

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
                            labels=tf.concat(train_labels, 0), logits=logits)
                            
predictions = tf.nn.softmax(logits)

```

Then the model is trained on a standard SGD optimizer (classical SGD, or Adam/RMSProp/others). 

>This model is pretty good, but it has some serious drawbacks. 

For one things, it's very shallow, and these linear layers aren't capable of capturing much complexity. For another things, the state vector is very crude, and information about older elements can be quickly lost as it optimizes for shorter-term memory. These problems are rectified by the LSTM (Long Short-Term Memory) model.

...

Like in convolution networks, the ability of such neural networks to summarize and perform "unsupervised learning" is quite extraordinary, and visualizing the activations of a fully trained LSTM can be linguistically fascinating.
