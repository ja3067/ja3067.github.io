---
layout: post
title: "Effective RNNs: A Guide to Recurrent Neural Networks in Tensorflow"
use_math: true
excerpt: Humans are great at contextualizing information. Our neural networks should be too.<br><br><img src="/images/RNN-unrolled.png"><br><br>
---

<img src="/images/RNN-unrolled.png"><br><br>

I woke up the other day, rather bleary-eyed, and sat down at my computer to check out the RNN I had thrown together the previous evening and left running overnight on the CPU of my Macbook. What I found was, to put it midly, surprising.

```
CAPULET:
Come, come in, we will tell thee, as reason
To the stately royaltys but little looks,
Where did he stoop and notice thee.

LUSLIS:
To my way, in the thing I did make the contents
That thyself that dissolvath you.

ROMEO:
Undowses, but he shall resolve to be so.

BUCKINGHAM:
That we will find me Henry, his men.

First Musician:
Sir, here’s a pretty thing, our soldiers’ wounds
are from the infants murderd weeping for
thy carelessness.
```

Okay, sure, it's not really "Shakespeare-Shakespeare", but at a first glance, as a STEM student, it could pass as an obscure quote from one of his later plays. And it wasn't just that quote, but page after page of faux-Shakespeare from my hastily written network. Andrej Karpathy's famous essay on the subject, [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), is aptly named: *RNNs are uncanny, strange, unpredictable, and sometimes beautiful*. You can find my char-rnn and many more pages from this and other datasets in my [Github LSTM repository](https://github.com/ja3067/char-rnn-tensorflow).

RNNs, or Recurrent Neural Networks, are networks capable of sequential learning. Humans are amazing at learning because we learn in context; we can remember and contextualize a word in a sentence that last appeared pages ago. **Our neural networks should be able to do the same thing.**

RNN inputs are sequences, and one thing you soon discover is that **just about anything can be a sequence**. It's easy to see that words and sentences are sequential, but RNNs can also be used to generate images, write music, and improve classification algorithms. Many [recent computer vision networks](https://arxiv.org/abs/1412.7755) have used RNNs to help their networks make smarter choices about where to look and what to care about.

Despite the apparent complexity of a sequence based model, they tend to be structurally simpler than the large convolution networks used in image classification. The simplest RNN model is shown below: the first element in the sequence is fed into the network, which produces an output *h* and a state vector, represented by the arrow exiting and reentering the body, which is fed back into the same network in time for the next input to be processed. Since the weights inside the neuron "body" control both how it interprets and generates the state vector, it can learn via backpropogation how to summarize and decode important information about previous elements in the sequence.

![Unrolled RNN](/images/rnn.jpg)

The mathematics of such a model is quite simple. The input, say, a character (a-z), is one-hot encoded, i.e. represented as a vector as long as the number of unique inputs (26, in this case), with all zeros except at the position corresponding to the character in question. The state and input vectors are put through linear layers (multiplied by a matrix w and shifted by a bias b), and then are added to each other and normalized.

$$
\begin{align*}
  &s_t = \tanh(w_i\cdot i_t + w_s\cdot s_{t-1} + b_i + b_s)\\
  &h_t = w_o\cdot s_t + b_o
\end{align*}
$$

The programing is not much more complicated.

## Implementation in Tensorflow

First, we define the RNN_cell, which takes our inputs and the previous state, and returns an output and state. We concatenatee the input and state, apply the linear layer, and return a new state variable. The output has to be resized back to the size of the vocabulary, so we push it through yet another linear layer. 


```python
def RNN_cell(input, state):

  wi = tf.get_variable("wi", [input_size, rnn_size], initializer=...) # weight and bias for input
  bi = tf.get_variable("bi", [rnn_size], initializer=...)
  
  ws = tf.get_variable("ws", [rnn_size, rnn_size], initializer=...) # weight and bias for state vector
  bs = tf.get_variable("bs", [rnn_size], initializer=...)
  
  wo = tf.get_variable("wo", [rnn_size, vocab_size], initializer=...) # weight and bias for decoding RNN output
  bo = tf.get_variable("bo", [vocab_size], initializer=...)

  state = tf.nn.tanh(tf.nn.xw_plus_b(input, wi, bi) + tf.nn.xw_plus_b(state, ws, bs))
  output = tf.nn.xw_plus_b(state, wo, bo)
  
  return state, output
```
 
Then we unpack the the training data and create separate data and label tensors. The labels are just going to be the inputs shifted by one time stamp, since we want our RNN to predict the next element in a sequence. That elegance is what makes RNNs so powerful and easy to use
  
```python
train_data = list() # here we take a sequence of characters from the trianing data and store them in a list
for _ in range(sequence_length + 1):
  train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))
inputs = train_data[:sequence_length]
train_labels = train_data[1:]
```

Then we create an initial state variable, and repeatedly apply the RNN_cell to our inputs, modifying the state variable each time. At the end, we convert our output list into a tensor, and apply a standard loss function.

```python
state = tf.constant(shape=[rnn_size]) # initial state variable

outputs = [] # list for storing outputs
 
for element in inputs:
  with tf.variable_scope("RNN") as scope:
    state, output = RNN_cell(element, state)
    scope.reuse_variables()
    outputs.append(output)

logits = tf.pack(outputs) # convert list into tensor

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
                            labels=tf.concat(train_labels, 0), logits=logits)
                            
predictions = tf.nn.softmax(logits)

```

Then the model is trained on a standard SGD optimizer (classical SGD, or Adam/RMSProp/others). 

## This model is pretty good, but it has some serious drawbacks. There must be a better way.

For one things, our very shallow, and these linear layers aren't capable of capturing much complexity. For another things, the state vector is very crude, and information about older elements can be quickly lost as it optimizes for shorter-term memory. These problems are rectified by the LSTM (Long Short-Term Memory) model.

...

Like in convolution networks, the ability of such neural networks to summarize and perform "unsupervised learning" is quite extraordinary, and visualizing the activations of a fully trained LSTM can be linguistically fascinating.
